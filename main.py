#!/usr/bin/env python
"""Encoder-Decoder with search for machine translation.

In this demo, encoder-decoder architecture with attention mechanism
is used for machine translation. The attention mechanism is implemented
according to [BCB]_. The training data used is WMT15 Czech to English
corpus, which you have to download and put to your 'datadir' in the
config file. You might need to tokenize, clean and shuffle your data.
Dictionaries for source and target languages can be generated by using
GroundHog preprocessing pipeline.
https://github.com/lisa-groundhog/GroundHog/tree/master/experiments/
nmt#data-preparation

.. [BCB] Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio. Neural
   Machine Translation by Jointly Learning to Align and Translate.
"""

import argparse
import logging
import pprint

import config
import stream
from examples.machine_translation import main

logger = logging.getLogger(__name__)

# Get the arguments
parser = argparse.ArgumentParser()
parser.add_argument("--proto",  default="get_config_cs2en",
                    help="Prototype config to use for config")
args = parser.parse_args()

# Make config global, nasty workaround since parameterizing stream
# will cause erroneous picklable behaviour
# TODO: find a beter solution
config = getattr(config, args.proto)()


# dictionary mapping stream name to stream getters
streams = {'cs-en': stream}


if __name__ == "__main__":
    logger.info("Model options:\n{}".format(pprint.pformat(config)))
    tr_stream, dev_stream = [streams[config['stream']].masked_stream,
                             streams[config['stream']].dev_stream]
    main(config, tr_stream, dev_stream)
